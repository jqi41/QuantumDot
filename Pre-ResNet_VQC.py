#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Mar 21 09:24:01 2024

@author: junqi
"""

"""Charge stability diagram classification with Pre-X+VQC models. The Pre-X+VQC includes:
    1. PCA + VQC
    2. (PreTrained)-ResNet18 + VQC
    3. (PreTrained)-ResNet50 + VQC

Dataset: the configuration are generated by our simulator on a 50x50 square lattice for 
both noisy and noiseless cases.""" 

#### Helper Libraries 
import numpy as np
import matplotlib.pyplot as plt
import argparse

#### Machine learning related libraries:
import torch 
import torch.nn as nn             # Base class usedd to develop all neural network models 
import torchvision                # Introduce Pre-trained ResNet models

#### Torch Quantum Library
import torchquantum as tq    
from vqc import VQC        

#### from itertools import chain         
from torch.utils.data import DataLoader  # easy and organized data loading to the ML model
from torch.utils.data import Dataset     # for nice loadable dataset creation


### Setting the Meta-parameters for Pre-X+VQC models and its training
parser = argparse.ArgumentParser(description='Training a Pre-X+VQC model for charge stability diagram classification')
parser.add_argument('--save_path', metavar='DIR', default='models', help='saved model path')
parser.add_argument('--num_qubits', default=5, help='The number of qubits', type=int)
parser.add_argument('--batch_size', default=8, help='the batch size', type=int)
parser.add_argument('--num_epochs', default=30, help='The number of epochs', type=int)
parser.add_argument('--depth_vqc', default=2, help='The depth of VQC', type=int)
parser.add_argument('--lr', default=0.001, help='Learning rate', type=float)
parser.add_argument('--model_kind', metavar='DIR', default='ResNet50', help='PreTrained model (ResNet18 or ResNet50)')
parser.add_argument('--test_kind', metavar='DIR', default='rep', help='Assessing representation (rep) or generalization (gen) powers')

args = parser.parse_args()


### Detect if running on the clusters ###
#### Using CUDA:
torch.cuda.is_available()
print("is CUDA available?", torch.cuda.is_available())

if torch.cuda.is_available():
    device = torch.device("cuda:0")  # You can continue going on here, like cuda:1 cuda:2 ... etc
    print("Running on the GPU")
else:
    device = torch.device("cpu")
    print("Running on the CPU")
    
    
### 1. Data preparation 
with open('mlqe_2023_edx/week1/dataset/csds.npy', 'rb') as f:
    data_noisy = np.load(f)

with open('mlqe_2023_edx/week1/dataset/csds_noiseless.npy', 'rb') as f:
    data_clean = np.load(f)
        
with open('mlqe_2023_edx/week1/dataset/labels.npy', 'rb') as f:
    labels = np.load(f)
    
    
#### Now that we know how our dataset looks like, we need to make it readable for PyTorch
class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = torch.Tensor(data)
        self.labels = torch.Tensor(labels)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        data_idx = self.data[idx]
        label = self.labels[idx].type(torch.LongTensor)
        return data_idx, label
    

### 2. Building up a Pre-X+VQC model
#### Building a VQC model after a classical Pre-X model
class PostVQC(nn.Module):
    def __init__(self, num_qubits=8, num_qlayers=2):
        super(PostVQC, self).__init__()
        self.num_qubits = num_qubits
        self.flatten = nn.Flatten()
        if args.model_kind == 'ResNet50':
            self.pre_net = nn.Linear(2048, num_qubits)
        elif args.model_kind == 'ResNet18':
            self.pre_net = nn.Linear(512, num_qubits)
        else:
            TypeError('Only the pre-trained models ResNet18 and ResNet50 are supported.')
        self.vqc = VQC(n_wires=num_qubits, n_qlayers=num_qlayers)
    
    def forward(self, x):
        self.dev = tq.QuantumDevice(n_wires=self.num_qubits, bsz=x.shape[0])
        x = self.flatten(x)
        pre_out = self.pre_net(x) 

        q_in = torch.sigmoid(pre_out) * np.pi / 2.0
        q_out = self.vqc(q_in, self.dev)
        
        #### The first 2 measurements are taken as the output 
        return q_out[:, :2]
    
    
#### Define the training function: make prediction on data set batch, backpropagate the error and adjust model parameters
def train(dataloader, model, loss_fn, optimizer, train_loss, train_acc, test_kind='rep'):
    num_batches = len(dataloader)
    size = len(dataloader.dataset)
    model.train()
    running_loss, correct = 0, 0
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)
        X = X.view(-1, 1, 50, 50)
        # compute a prediction error
        pred = model(X)
        loss = loss_fn(pred, y)
        
        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Collect the accuracy:
        running_loss += loss.item()
        correct += (pred.argmax(1)==y).type(torch.float).sum().item()
        
        if batch % 25 == 0:
            loss, current = loss.item(), batch*len(X)
            print(f"loss: {loss:>7f} [{current:>5d} / {size:>5d}]")
    running_loss /= num_batches
    correct /= size
    
    train_acc.append(correct)
    train_loss.append(running_loss)


#### Checking performance against the test data set
def test(dataloader, model, loss_fn, test_loss, test_acc, test_kind='rep'):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    running_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            X = X.view(-1, 1, 50, 50)
            pred = model(X)
            running_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1)==y).type(torch.float).sum().item()
    running_loss /= num_batches
    correct /= size
    
    test_acc.append(correct)
    test_loss.append(running_loss)
    print(f"Test Error: \n Accuracy {(100*correct):>0.1f}%, Avg loss:{running_loss:>8f}\n")


def create_acc_loss_graph(train_acc, train_loss, test_acc, test_loss):
    fig, axes = plt.subplots(ncols=2, nrows=1, dpi=300)
    fig.set_size_inches(9, 3)
    ax1, ax2 = axes[0], axes[1]
    
    ax1.plot(train_acc, '-o', label="train", markersize=4)
    ax1.plot(test_acc, '--+', label="test", markersize=4)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend(loc=3)
    
    ax2.plot(train_loss, '-o', label='train', markersize=4)
    ax2.plot(test_loss, '--+', label='test', markersize=4)
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend(loc=1)
    
    # Plot the parameters
    ax1.set_ylim(0, np.max([np.max(train_acc), np.max(test_acc)]) + 0.1)
    ax2.set_ylim(-0.1, np.max([np.max(train_loss), np.max(test_loss)]) + 0.1)
    ax1.grid(True, which='both', linewidth=0.1)
    ax2.grid(True, which='both', linewidth=0.1)
    plt.tight_layout()
    plt.show()



if __name__ == "__main__":
    
    ### 1. Data preparation
    if args.test_kind == 'rep':
        data = data_clean
    elif args.test_kind == 'gen':
        data = data_noisy
    else:
        TypeError("Only 'rep' are 'gen' types are supported.")
        
    #### Let's keep 20 percent of them for the test set and 80 percent for the training set and fix the batch size
    dataset = CustomDataset(data, labels)
    trainset, testset = torch.utils.data.random_split(dataset, (int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)))

    trainloader = DataLoader(trainset, batch_size = args.batch_size)
    testloader = DataLoader(testset, batch_size = args.batch_size)

    #### Then, we can check the shape of the dataset
    for X, y in trainloader:
        print(f"Shape of X: {X.shape}")
        print(f"Shape of y: {y.shape} {y.dtype}")
        break

    ### 2. Set up the model 
    if args.model_kind == 'ResNet50':
        PreX_vqc = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)
        print('Building up a Pre-ResNet50+VQC model:')
    elif args.model_kind == 'ResNet18':
        print('Building up a Pre-ResNet18+VQC model:')
        #PreX_vqc = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)
        PreX_vqc = torchvision.models.resnet18()
    else:
        TypeError("The pre-trained model should be ResNet18 or ResNet50")
        
    for param in PreX_vqc.parameters():
        param.requires_grad = False
    PreX_vqc.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    PreX_vqc.fc = PostVQC(args.num_qubits, args.depth_vqc)
    PreX_vqc = PreX_vqc.to(device)
        
    #### Optimizing the model parameters
    #### To train the model, we need a LOSS FUNCTION and an OPTIMIZER
    loss_fn = nn.CrossEntropyLoss()  # we use Cross-Entropy as loss
    optimizer = torch.optim.Adam(PreX_vqc.parameters(), lr=args.lr) # The optimizer is Adam
    
    #### Define the training and test loss accumulation
    train_loss_PreX_vqc = []
    train_acc_PreX_vqc = []
    test_loss_PreX_vqc = []
    test_acc_PreX_vqc = []
    
    for t in range(args.num_epochs):
        print(f"Epoch {t+1}\n --------------------")
        train(trainloader, PreX_vqc, loss_fn, optimizer, train_loss_PreX_vqc, train_acc_PreX_vqc)
        test(testloader, PreX_vqc, loss_fn, test_loss_PreX_vqc, test_acc_PreX_vqc)
    print("Done!")

    create_acc_loss_graph(train_acc_PreX_vqc, train_loss_PreX_vqc, test_acc_PreX_vqc, test_loss_PreX_vqc)
    