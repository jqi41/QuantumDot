#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Created on Wed Dec 27 15:36:50 2023
Supervised Learning for Quantum Dot Configuration Tuning

@author: junqi
"""

"""Charge stability diagram classification with dense Neural Networks
In this notebook, we learn how to classify the charge stability diagram of a single 
and double quantum dots with the help of Neural Networks. The NN model that we are 
considering here are the fully connected neural networks and the Convolutional Neural
Networks. 

Dataset: the configurations are generated by our simulator on a 50x50 square lattice for 
both noisy and noiseless cases.""" 

# Helper Libraries 
import numpy as np
import matplotlib.pyplot as plt
import argparse

# Machine learning related libraries:
import torch 
import torch.nn as nn             # Base class used to develop all neural network models

import torchquantum as tq

# from itertools import chain     # append two range() functions
from torch.utils.data import DataLoader  # easy and organized data loading to the ML model
from torch.utils.data import Dataset     # for nice loadable dataset creation

from vqc import VQC
from sklearn.decomposition import PCA


parser = argparse.ArgumentParser(description='Training a PCA+VQC model for charge stability diagram classification')
parser.add_argument('--save_path', metavar='DIR', default='models', help='saved model path')
parser.add_argument('--num_qubits', default=8, help='The number of qubits', type=int)
parser.add_argument('--batch_size', default=8, help='the batch size', type=int)
parser.add_argument('--num_epochs', default=30, help='The number of epochs', type=int)
parser.add_argument('--depth_vqc', default=2, help='The depth of VQC', type=int)
parser.add_argument('--lr', default=0.001, help='Learning rate', type=float)
parser.add_argument('--model_kind', metavar='DIR', default='PCA', help='Only PCA+VQC is supported')
parser.add_argument('--test_kind', metavar='DIR', default='gen', help='Assessing representation (rep) or generalization (gen) powers')

args = parser.parse_args()



####### Detect if running on the clusters #######
# Using CUDA:
torch.cuda.is_available()
print("is CUDA available?", torch.cuda.is_available())

# Setting a Flag
device = torch.device("cuda:0")

if torch.cuda.is_available():
    device = torch.device("cuda:0")  # You can continue going on here, like cuda:1 cuda:2 ... etc
    print("Running on the GPU")
else:
    device = torch.device("cpu")
    print("Running on the CPU")
    

with open('mlqe_2023_edx/week1/dataset/csds.npy', 'rb') as f:
    data_noisy = np.load(f)

with open('mlqe_2023_edx/week1/dataset/csds_noiseless.npy', 'rb') as f:
    data_clean = np.load(f)
    
with open('mlqe_2023_edx/week1/dataset/labels.npy', 'rb') as f:
    labels = np.load(f)


""" Let's visualize our data to see how each of phase diagram look like with their corresponding label. """
fig, ax = plt.subplots(1, 10, figsize=(20, 10))
for index, d in enumerate(data_clean[np.random.choice(len(data_clean), size=10)]):
    ax[index].imshow(data_clean[index])
    ax[index].axis('off')
    ax[index].set_title(f'Label: {(labels[index])}')
plt.show()
plt.close()


""" Data preparation """
# Now that we know what our dataset looks like, we need to make it readable for PyTorch
class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = torch.Tensor(data)
        self.labels = torch.Tensor(labels)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        data_idx = self.data[idx]
        label = self.labels[idx].type(torch.LongTensor)
        return data_idx, label
    
# Let's keep 20 percent of them for the test set and 80 percent for the training set and fix the batch size
if args.test_kind == 'gen':
    dataset = CustomDataset(data_noisy, labels)
else:
    dataset = CustomDataset(data_clean, labels)
trainset, testset = torch.utils.data.random_split(dataset, (int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)))

batch_size = args.batch_size
trainloader = DataLoader(trainset, batch_size = batch_size)
testloader = DataLoader(testset, batch_size = batch_size)


# Then, we can check the shape of the dataset
for X, y in trainloader:
    print(f"Shape of X: {X.shape}")
    print(f"Shape of y: {y.shape} {y.dtype}")
    print(y)
    break

# Setup the model 
"""Finally, we can set up the model! 
One example of how to do that: input size is 50 x 50 = 2500, followed by one fully
connected layer with ReLU activation function and binary (1/0) output. 

We define a class for the neural network. torch requires this class to have a.forward(x)
method, and it is a conversion to define the network during hte initialization of the class.
"""

# Get CPU or GPU device for the training work
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

class PCA_VQC(nn.Module):
    def __init__(self, num_qubits=8, depth_vqc=2):
        super(PCA_VQC, self).__init__()
        self.flatten = nn.Flatten()
        self.num_qubits = num_qubits
        self.pca = PCA(n_components=num_qubits)
        self.vqc = VQC(n_wires=num_qubits, n_qlayers=depth_vqc)
    
    def forward(self, x):
        self.dev = tq.QuantumDevice(n_wires=self.num_qubits, bsz=x.shape[0])
        x = self.flatten(x)
        pre_out = torch.tensor(self.pca.fit_transform(x))
        q_in = torch.sigmoid(pre_out) * np.pi / 2.0
        q_out = self.vqc(q_in, self.dev)
        
        return q_out[:, :2]
    
# Here we initialize an instance of the class
model = PCA_VQC(args.num_qubits, args.depth_vqc).to(device)
# print the status of the model
# the print command is inherited from nn.Module in the definition of the network
print(model)

# We can see the number of trainable parameters
pytorch_total_params = sum(p.numel() for p in model.parameters())
print(f"Trainable params: {pytorch_total_params}")


""" Compile the model

Now that the model is defined, we need to train our model. But before doing so, there are a few
details we need to specify.
1. loss function: we need to choose what function we want our model to minimize e.g., mean square, cross-entropy.
2. Optimization method: how we want to update the weights e.g., stochastic gradient descent, Adam, etc. 
3. Metrics: sokme quantity we want to keep track of while we are training, e.g., value of the loss function or the accuracy of the model
"""

# Optimizing the model parameters
# To train the model, we need a LOSS FUNCTION and an OPTIMIZER
loss_fn = nn.CrossEntropyLoss()  # we use Cross-Entropy as loss
optimizer = torch.optim.Adam(model.parameters(), lr=args.lr) # The optimizer is Adam

# Define the training and test loss accumulation
train_loss_dnn = []
train_acc_dnn = []
test_loss_dnn = []
test_acc_dnn = []


# Define the training function: make prediction on data set batch,
# Backpropagate the error and adjust model parameters
def train(dataloader, model, loss_fn, optimizer, train_loss, train_acc):
    num_batches = len(dataloader)
    size = len(dataloader.dataset)
    model.train()
    running_loss, correct = 0, 0
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)
        
        # compute a prediction error
        pred = model(X)
        loss = loss_fn(pred, y)
        
        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Collect the accuracy:
        running_loss += loss.item()
        correct += (pred.argmax(1)==y).type(torch.float).sum().item()
        
        if batch % 25 == 0:
            loss, current = loss.item(), batch*len(X)
            print(f"loss: {loss:>7f} [{current:>5d} / {size:>5d}]")
    running_loss /= num_batches
    correct /= size
    
    train_acc.append(correct)
    train_loss.append(running_loss)


# Checking performance against the test data set
def test(dataloader, model, loss_fn, test_loss, test_acc):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    running_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            running_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1)==y).type(torch.float).sum().item()
    running_loss /= num_batches
    correct /= size
    
    test_acc.append(correct)
    test_loss.append(running_loss)
    print(f"Test Error: \n Accuracy {(100*correct):>0.1f}%, Avg loss:{running_loss:>8f}\n")
    

def create_acc_loss_graph(train_acc, train_loss, test_acc, test_loss):
    fig, axes = plt.subplots(ncols=2, nrows=1, dpi=300)
    fig.set_size_inches(9, 3)
    ax1, ax2 = axes[0], axes[1]
    
    ax1.plot(train_acc, '-o', label="train", markersize=4)
    ax1.plot(test_acc, '--+', label="test", markersize=4)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend(loc=3)
    
    ax2.plot(train_loss, '-o', label='train', markersize=4)
    ax2.plot(test_loss, '--+', label='test', markersize=4)
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend(loc=1)
    
    # Plot the parameters
    ax1.set_ylim(0, np.max([np.max(train_acc), np.max(test_acc)]) + 0.1)
    ax2.set_ylim(-0.1, np.max([np.max(train_loss), np.max(test_loss)]) + 0.1)
    ax1.grid(True, which='both', linewidth=0.1)
    ax2.grid(True, which='both', linewidth=0.1)
    plt.tight_layout()
    plt.show()
    

if __name__ == "__main__":
    for t in range(args.num_epochs):
        print(f"Epoch {t+1}\n --------------------")
        train(trainloader, model, loss_fn, optimizer, train_loss_dnn, train_acc_dnn)
        test(testloader, model, loss_fn, test_loss_dnn, test_acc_dnn)
    print("Done!")

    create_acc_loss_graph(train_acc_dnn, train_loss_dnn, test_acc_dnn, test_loss_dnn)
